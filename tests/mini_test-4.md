4.1 По данным: `id пользователя, домен` посчитайте похожести доменов. Т.е. пары доменов, которые встречаются у наибольшего числа пользователей. 

Отсортируйте пары доменов по схожести (в порядке ее убывания). Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.2	Исходные данные: `id пользователя, id сессии, url`.

Для каждого пользователя посчитайте число сессий и уникальных доменов. Cессия -– это посещения человеком страниц (любых страниц, а не страниц одного сайта) с перерывами не более, чем на 30 минут.

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.3	Исходные данные: `id пользователя, id сессии, url`. 

Для каждого пользователя посчитайте среднее уникальных доменов на сессию. Сессия -– это посещения человеком страниц (любых страниц, а не страниц одного сайта) с перерывами не более, чем на 30 минут. 

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.4	Исходные данные: `id пользователя, gender, url`. 

Для каждого домена из url посчитайте число уникальных пользователей каждого пола. 

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.5	Для корпуса текстов (например, набор статей википедии) посчитайте `tf*idf`. Т.е. исходные данные: `id статьи, текст`. Величина tf зависит от слова t и документа d и вычисляется по формуле:

	tf(t, d) = nt/N,

где nf –- число появлений слова t в документе d, N -– число слов в d.

Idf зависит от слова и равна:

	idf(t) = D/D(t)

где D –- число документов, D(t) –- число документов, содержащих t.

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.6	Исходные данные – посещения пользователями страниц: 
`userid, timestamp, url, date`

Нужно отфильтровать за два дня -- d1 и d2 такие url’ы, на которых было > 100 посещений в день. Это будут множества U1 и U2. 

Далее за d2 отобрать только новые, которых не было в первый день: U2\U1. 

На этом множестве отобрать все события за d1. (Т.е. таким образом, мы отбираем пользователей, которые посещали страницы, которые стали популярными лишь на следующий день). 

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.7	Есть данные о посещениях страниц -– events:
`userid, timestamp, url, date`

и данные о новостных кластерах (страницы, которые на разных сайтах содержат одну и ту же новость образуют новостной кластер) –- news:
`url, cluster_id`

Постройте гистограмму количества кластеров от количества посещений (т.е. n1 кластеров с одним визитом, n2 с двумя и т.д.).

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.8	Есть данные о посещения пользователями страниц:
`userid, timestamp, url, browser`
Browser – это Chrome, Firefox, MSIE, Safari и т.д. При этом один пользователь может приходить из нескольких браузеров. 

Соберите статистику посещений доменов для всех пользователей, которые хотя бы раз появлялись с браузером Chrome в виде:
`userid, domain, count`

Опишите схему решения для Hadoop.

----------
<P style="page-break-before: always">

4.9 Нам нужно считать ТОП N самых популярных слов в тексте на MapReduce. Как это можно эффективно реализовать (взяв за основу стандартную программу WordCount), зная N заранее (N порядка 1000). 

----------
<P style="page-break-before: always">

4.10 Слова считаем равными с точностью до перестановки букв (abc=bca). Во входном тексте найдите наиболее популярные перестановки, также посчитайте число исходных слов для перестановки. Т.е. если ‘abc’ встретилось 50 раз и ‘bca’ 50 и других слов из этих букв не было, то в результате будет: abc 100 2.

Отсортируйте результат по убыванию популярности перестановки, с одинаковой популярностью – лексикографически. Исходные данные очищайте от знаков пунктуации, фильтруйте слова короче 3х символов.

Опишите схему решения для Hadoop.
